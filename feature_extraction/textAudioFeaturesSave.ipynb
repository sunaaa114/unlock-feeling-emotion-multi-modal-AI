{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a377c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josun\\anaconda3\\envs\\newbie\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import os\n",
    "#from python_speech_features import mfcc\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv2D, Flatten, concatenate, Input, Dropout, MaxPooling2D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import pickle\n",
    "import whisper\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e735e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cc980",
   "metadata": {},
   "source": [
    "## JSON에서 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ce876c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonText(json_directory):\n",
    "    json_files = sorted(os.listdir(json_directory))\n",
    "    \n",
    "    emotion = ['기쁨', '놀람', '중립', '공포', '혐오', '분노', '슬픔']\n",
    "\n",
    "    emotion_to_array = {\n",
    "        '기쁨': np.array([1,0,0,0,0,0,0]),\n",
    "        '놀람': np.array([0,1,0,0,0,0,0]),\n",
    "        '중립': np.array([0,0,1,0,0,0,0]),\n",
    "        '공포': np.array([0,0,0,1,0,0,0]),\n",
    "        '혐오': np.array([0,0,0,0,1,0,0]),\n",
    "        '분노': np.array([0,0,0,0,0,1,0]),\n",
    "        '슬픔': np.array([0,0,0,0,0,0,1])\n",
    "    }\n",
    "    \n",
    "    texts = {'combined_text':[]}\n",
    "    label = []\n",
    "    for file in json_files:\n",
    "        jfile = os.path.join(json_directory, file)\n",
    "        with open(jfile, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        lb = data[\"화자정보\"][\"Emotion\"]\n",
    "        if lb == \"Anxious\" or lb == \"Hurt\" or lb == \"Embarrassed\":\n",
    "            continue\n",
    "        label.append(emotion_to_array[lb])\n",
    "        text_content = data[\"전사정보\"][\"TransLabelText\"]\n",
    "        #text_content = np.array(text_content)\n",
    "        texts['combined_text'].append(text_content)\n",
    "        \n",
    "    print(len(texts['combined_text']))\n",
    "    print(label[1])\n",
    "    return texts, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "a0426353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\labeling\\TL2\\7.중립\\0002_G1A4E7S0C0_LYT\n",
      "1990\n",
      "[0 0 1 0 0 0 0]\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\labeling\\TL2\\7.중립\\0003_G1A4E7S0C0_JCH\n",
      "1987\n",
      "[0 0 1 0 0 0 0]\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\labeling\\TL2\\7.중립\\0005_G1A3E7S0C0_LJB\n",
      "2286\n",
      "[0 0 1 0 0 0 0]\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\labeling\\TL2\\7.중립\\0006_G1A3E7S0C0_LHJ\n",
      "1992\n",
      "[0 0 1 0 0 0 0]\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\labeling\\TL2\\7.중립\\0008_G1A3E7S0C0_JYC\n",
      "1992\n",
      "[0 0 1 0 0 0 0]\n",
      "\n",
      " 1\n",
      "10247\n",
      "10247\n"
     ]
    }
   ],
   "source": [
    "json_dts = \"D:\\\\Speech synthesis data by emotion and speech style\\\\data\\\\1.Training\\\\labeling\\\\TL2\\\\7.중립\"\n",
    "json_dts_sort = sorted(os.listdir(json_dts))\n",
    "\n",
    "texts = {\"combined_text\":[]}\n",
    "labels = []\n",
    "for i in range(0, 5):\n",
    "    json_directory = os.path.join(json_dts, json_dts_sort[i])\n",
    "    print(json_directory)\n",
    "    text, label = jsonText(json_directory)\n",
    "    \n",
    "    texts['combined_text'].extend(text['combined_text'])\n",
    "    labels = labels + label\n",
    "    \n",
    "print(\"\\n\", len(texts))\n",
    "print(len(texts['combined_text']))\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add787b",
   "metadata": {},
   "source": [
    "## 텍스트 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1b671011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def add_kobert_embeddings(df, text_column):\n",
    "    embeddings_numpy = []\n",
    "    for text in df[text_column]:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_tensor = torch.tensor([token_ids])\n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            tokens_tensor.to(device)\n",
    "            outputs = model(tokens_tensor)\n",
    "            sentence_embedding = outputs[0][:, 0, :].squeeze().cpu().numpy()\n",
    "        embeddings_numpy.append(sentence_embedding)\n",
    "    df['kobert_embeddings_numpy'] = embeddings_numpy\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "b7a1a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10247\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "df = add_kobert_embeddings(texts, 'combined_text')\n",
    "\n",
    "# 리스트에 있는 숫자의 인덱스에 해당하는 값들을 가져와서 리스트로 만듦\n",
    "text_features = df['kobert_embeddings_numpy']\n",
    "\n",
    "print(len(text_features))\n",
    "print(len(text_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4567a",
   "metadata": {},
   "source": [
    "## 오디오 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6128d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioFeature(audio_directory):\n",
    "    audio_files = sorted(os.listdir(audio_directory))\n",
    "    \n",
    "    audio_df = pd.DataFrame(columns=['feature'])\n",
    "    i = 0\n",
    "    for filename in audio_files:\n",
    "        if filename.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(audio_directory, filename)\n",
    "            X, sample_rate = librosa.load(audio_path,duration=1,sr=22050*2)\n",
    "            sample_rate = np.array(sample_rate)\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=25), axis=0)\n",
    "            feature = mfccs\n",
    "            audio_df.loc[i] = [feature]\n",
    "            i=i+1\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    audio_df2 = pd.DataFrame(columns=['normalized'])\n",
    "    for k in range(len(audio_df)):\n",
    "        features = audio_df.loc[k, 'feature'].reshape(1, -1)\n",
    "        features_transposed = features.T\n",
    "\n",
    "        # 정규화 적용\n",
    "        scaled_features_transposed = scaler.fit_transform(features_transposed)\n",
    "\n",
    "        # 다시 전치하여 shape을 (n_samples, n_features)로 변경\n",
    "        scaled_features = scaled_features_transposed.T\n",
    "        scaled_features = scaled_features.flatten()\n",
    "        scaled_features.tolist()\n",
    "        audio_df2.loc[k, 'normalized'] = scaled_features.tolist()\n",
    "\n",
    "    audio_3 = pd.DataFrame(audio_df2['normalized'].values.tolist())\n",
    "    audio_3 = audio_3.fillna(0)\n",
    "    \n",
    "    # 모든 행을 배열로 변환\n",
    "    audio_features = []\n",
    "    for index, row in audio_3.iterrows():\n",
    "        row_array = np.array(row)\n",
    "        audio_features.append(row_array)\n",
    "        \n",
    "    print(len(audio_features))\n",
    "    \n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "830aed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\source\\TS1\\3.분노\\0025_G2A4E3S0C0_GJY\n",
      "1990\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\source\\TS1\\3.분노\\0026_G2A3E3S0C0_JSH\n",
      "1988\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\source\\TS1\\3.분노\\0027_G2A3E3S0C0_KYK\n",
      "1990\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\source\\TS1\\3.분노\\0028_G2A3E3S0C0_KJS\n",
      "1988\n",
      "D:\\Speech synthesis data by emotion and speech style\\data\\1.Training\\source\\TS1\\3.분노\\0029_G2A4E3S0C0_KJE\n",
      "1989\n",
      "\n",
      " 9945\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "audio_dts = \"D:\\\\Speech synthesis data by emotion and speech style\\\\data\\\\1.Training\\\\source\\\\TS1\\\\3.분노\"\n",
    "audio_dts_sort = sorted(os.listdir(audio_dts))\n",
    "\n",
    "audio_features = []\n",
    "\n",
    "\n",
    "for i in range(20, 25):\n",
    "    audio_directory = os.path.join(audio_dts, audio_dts_sort[i])\n",
    "    print(audio_directory)\n",
    "    audio_ft = audioFeature(audio_directory)\n",
    "    \n",
    "    audio_features += audio_ft\n",
    "\n",
    "print(\"\\n\", len(audio_features))\n",
    "print(len(audio_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb671714",
   "metadata": {},
   "source": [
    "## 특징 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "3e14e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 지정\n",
    "file_path1 = 'D:\\\\text_audio_features\\\\기쁨\\\\textF6.pkl'\n",
    "file_path2 = \"D:\\\\text_audio_features\\\\기쁨\\\\audioF6.pkl\"\n",
    "file_path3 = \"D:\\\\text_audio_features\\\\기쁨\\\\y6.pkl\"\n",
    "\n",
    "# 파일에 변수 저장\n",
    "with open(file_path1, 'wb') as file:\n",
    "    pickle.dump(text_features, file)\n",
    "    \n",
    "with open(file_path2, 'wb') as file:\n",
    "    pickle.dump(audio_features, file)\n",
    "    \n",
    "with open(file_path3, 'wb') as file:\n",
    "    pickle.dump(labels, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "2b5c7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textF: 10392\n",
      "audioF: 10392\n",
      "y: 10392\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# 저장된 변수를 다시 불러오는 코드\n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\textF1.pkl', 'rb') as file:\n",
    "    textF = pickle.load(file)\n",
    "\n",
    "    # 저장된 변수를 다시 불러오는 코드\n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\audioF1.pkl', 'rb') as file:\n",
    "    audioF = pickle.load(file)\n",
    "\n",
    "# 저장된 변수를 다시 불러오는 코드\n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\y1.pkl', 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "\n",
    "print(\"textF:\", len(textF))\n",
    "print(\"audioF:\", len(audioF))\n",
    "print(\"y:\", len(y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
