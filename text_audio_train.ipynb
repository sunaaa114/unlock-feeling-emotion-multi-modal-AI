{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b691d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv2D, Flatten, concatenate, Input, Dropout, MaxPooling2D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from TextAudioMultimodalClassifier7 import TextAudioMultimodalClassifier\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a555d8",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "942ddf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1~6까지\n",
    "n = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e873730",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\textF' + n + '.pkl', 'rb') as file:\n",
    "    happy_textF = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\textF' + n + '.pkl', 'rb') as file:\n",
    "    sad_textF = pickle.load(file)\n",
    "    \n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\textF' + \"7\" + '.pkl', 'rb') as file:\n",
    "    happy_textF7 = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\textF' + \"7\" + '.pkl', 'rb') as file:\n",
    "    sad_textF7 = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\분노\\\\textF' + n + '.pkl', 'rb') as file:\n",
    "    angry_textF = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\중립\\\\textF' + n + '.pkl', 'rb') as file:\n",
    "    normal_textF = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57db8513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39901"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textF = np.concatenate([happy_textF, sad_textF, angry_textF, normal_textF])\n",
    "textF = textF.astype('float32')\n",
    "len(textF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d835a7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57302"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textF = np.concatenate([happy_textF, happy_textF7, sad_textF, sad_textF7, angry_textF, normal_textF])\n",
    "textF = textF.astype('float32')\n",
    "len(textF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0d5d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "textF6 = textF[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68eeb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFF = np.concatenate([textF1, textF2, textF3, textF4, textF5, textF6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb660c7",
   "metadata": {},
   "source": [
    "260171개의 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce40996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260171"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e39170e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\audioF' + n + '.pkl', 'rb') as file:\n",
    "    happy_audioF = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\audioF' + n + '.pkl', 'rb') as file:\n",
    "    sad_audioF = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\audioF' + \"7\" + '.pkl', 'rb') as file:\n",
    "    happy_audioF7 = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\audioF' + \"7\" + '.pkl', 'rb') as file:\n",
    "    sad_audioF7 = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\분노\\\\audioF' + n + '.pkl', 'rb') as file:\n",
    "    angry_audioF = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\중립\\\\audioF' + n + '.pkl', 'rb') as file:\n",
    "    normal_audioF = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a0638c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40443"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioF = np.concatenate([happy_audioF, sad_audioF, angry_audioF, normal_audioF])\n",
    "audioF = audioF.astype('float32')\n",
    "len(audioF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "208a3ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57302"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioF = np.concatenate([happy_audioF, happy_audioF7, sad_audioF, sad_audioF7, angry_audioF, normal_audioF])\n",
    "audioF = audioF.astype('float32')\n",
    "len(audioF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df72e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioF4 = audioF[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3bcbcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioFF = np.concatenate([audioF1, audioF2, audioF3, audioF4, audioF5, audioF6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c38c7a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260171"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audioFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "203751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\y' + n + '.pkl', 'rb') as file:\n",
    "    happy_y = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\y' + n + '.pkl', 'rb') as file:\n",
    "    sad_y = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\기쁨\\\\y' + \"7\" + '.pkl', 'rb') as file:\n",
    "    happy_y7 = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\슬픔\\\\y' + \"7\" + '.pkl', 'rb') as file:\n",
    "    sad_y7 = pickle.load(file)\n",
    "    \n",
    "with open('D:\\\\text_audio_features\\\\분노\\\\y' + n + '.pkl', 'rb') as file:\n",
    "    angry_y = pickle.load(file)\n",
    "\n",
    "with open('D:\\\\text_audio_features\\\\중립\\\\y' + n + '.pkl', 'rb') as file:\n",
    "    normal_y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cca6b361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40443"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([happy_y, sad_y, angry_y, normal_y])\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5951530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57302"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([happy_y, happy_y7, sad_y, sad_y7, angry_y, normal_y])\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc239b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y4 = y[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "258de30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = np.concatenate([y1, y2, y3, y4, y5, y6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7991a379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260171"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69f18dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text_features, audio_features, labels):\n",
    "        self.text_features = text_features\n",
    "        self.audio_features = audio_features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text_input = torch.tensor(self.text_features[index], dtype=torch.float32)\n",
    "        audio_input = torch.tensor(self.audio_features[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        \n",
    "        return text_input, audio_input, label\n",
    "\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 512  # 배치 크기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e88db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 데이터와 검증 데이터로 나눔\n",
    "train_textF, val_textF, train_audioF, val_audioF, train_y, val_y = train_test_split(\n",
    "    textFF, audioFF, yy, test_size=0.2, random_state=42)\n",
    "\n",
    "# 훈련 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(text_features=train_textF, audio_features=train_audioF, labels=train_y)\n",
    "\n",
    "# 검증 데이터셋 인스턴스 생성\n",
    "val_dataset = CustomDataset(text_features=val_textF, audio_features=val_audioF, labels=val_y)\n",
    "\n",
    "# 데이터로더 생성 (훈련 데이터용)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 데이터로더 생성 (검증 데이터용)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9c5a5",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bb1d9418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextAudioMultimodalClassifier(\n",
       "  (text_layer): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (audio_layer): Linear(in_features=87, out_features=512, bias=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "input_dim_text = 768  # 텍스트 특징 개수\n",
    "input_dim_audio = 87  # 오디오 특징 개수\n",
    "num_classes = 7  # 감정 클래스 개수\n",
    "\n",
    "model = TextAudioMultimodalClassifier(input_dim_text, input_dim_audio, num_classes)\n",
    "\n",
    "'''# X에 있는 임베딩 값을 모델에 입력으로 사용\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor(X))'''\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 조기 종료를 위한 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# 모델 훈련\n",
    "num_epochs = 10000\n",
    "\n",
    "# 모델을 장치로 이동 (CPU 또는 GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "138072ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Accuracy: 0.4056, Validation Loss: 1.3315\n",
      "Epoch [2/10000], Accuracy: 0.4185, Validation Loss: 1.2877\n",
      "Epoch [3/10000], Accuracy: 0.4337, Validation Loss: 1.2691\n",
      "Epoch [4/10000], Accuracy: 0.4438, Validation Loss: 1.2559\n",
      "Epoch [5/10000], Accuracy: 0.4507, Validation Loss: 1.2452\n",
      "Epoch [6/10000], Accuracy: 0.4558, Validation Loss: 1.2365\n",
      "Epoch [7/10000], Accuracy: 0.4603, Validation Loss: 1.2292\n",
      "Epoch [8/10000], Accuracy: 0.4633, Validation Loss: 1.2235\n",
      "Epoch [9/10000], Accuracy: 0.4650, Validation Loss: 1.2182\n",
      "Epoch [10/10000], Accuracy: 0.4666, Validation Loss: 1.2136\n",
      "Epoch [11/10000], Accuracy: 0.4705, Validation Loss: 1.2095\n",
      "Epoch [12/10000], Accuracy: 0.4725, Validation Loss: 1.2055\n",
      "Epoch [13/10000], Accuracy: 0.4740, Validation Loss: 1.2018\n",
      "Epoch [14/10000], Accuracy: 0.4754, Validation Loss: 1.1983\n",
      "Epoch [15/10000], Accuracy: 0.4734, Validation Loss: 1.1960\n",
      "Epoch [16/10000], Accuracy: 0.4785, Validation Loss: 1.1930\n",
      "Epoch [17/10000], Accuracy: 0.4794, Validation Loss: 1.1903\n",
      "Epoch [18/10000], Accuracy: 0.4810, Validation Loss: 1.1879\n",
      "Epoch [19/10000], Accuracy: 0.4832, Validation Loss: 1.1849\n",
      "Epoch [20/10000], Accuracy: 0.4827, Validation Loss: 1.1829\n",
      "Epoch [21/10000], Accuracy: 0.4817, Validation Loss: 1.1820\n",
      "Epoch [22/10000], Accuracy: 0.4867, Validation Loss: 1.1778\n",
      "Epoch [23/10000], Accuracy: 0.4864, Validation Loss: 1.1755\n",
      "Epoch [24/10000], Accuracy: 0.4849, Validation Loss: 1.1748\n",
      "Epoch [25/10000], Accuracy: 0.4910, Validation Loss: 1.1721\n",
      "Epoch [26/10000], Accuracy: 0.4901, Validation Loss: 1.1692\n",
      "Epoch [27/10000], Accuracy: 0.4926, Validation Loss: 1.1677\n",
      "Epoch [28/10000], Accuracy: 0.4931, Validation Loss: 1.1652\n",
      "Epoch [29/10000], Accuracy: 0.4940, Validation Loss: 1.1638\n",
      "Epoch [30/10000], Accuracy: 0.4945, Validation Loss: 1.1616\n",
      "Epoch [31/10000], Accuracy: 0.4942, Validation Loss: 1.1601\n",
      "Epoch [32/10000], Accuracy: 0.4983, Validation Loss: 1.1594\n",
      "Epoch [33/10000], Accuracy: 0.4988, Validation Loss: 1.1565\n",
      "Epoch [34/10000], Accuracy: 0.5006, Validation Loss: 1.1551\n",
      "Epoch [35/10000], Accuracy: 0.4981, Validation Loss: 1.1537\n",
      "Epoch [36/10000], Accuracy: 0.4998, Validation Loss: 1.1512\n",
      "Epoch [37/10000], Accuracy: 0.5018, Validation Loss: 1.1492\n",
      "Epoch [38/10000], Accuracy: 0.5029, Validation Loss: 1.1477\n",
      "Epoch [39/10000], Accuracy: 0.5022, Validation Loss: 1.1472\n",
      "Epoch [40/10000], Accuracy: 0.5044, Validation Loss: 1.1463\n",
      "Epoch [41/10000], Accuracy: 0.5047, Validation Loss: 1.1443\n",
      "Epoch [42/10000], Accuracy: 0.5039, Validation Loss: 1.1425\n",
      "Epoch [43/10000], Accuracy: 0.5060, Validation Loss: 1.1423\n",
      "Epoch [44/10000], Accuracy: 0.5081, Validation Loss: 1.1397\n",
      "Epoch [45/10000], Accuracy: 0.5078, Validation Loss: 1.1380\n",
      "Epoch [46/10000], Accuracy: 0.5090, Validation Loss: 1.1388\n",
      "up\n",
      "Epoch [47/10000], Accuracy: 0.5052, Validation Loss: 1.1369\n",
      "Epoch [48/10000], Accuracy: 0.5103, Validation Loss: 1.1354\n",
      "Epoch [49/10000], Accuracy: 0.5093, Validation Loss: 1.1341\n",
      "Epoch [50/10000], Accuracy: 0.5093, Validation Loss: 1.1324\n",
      "Epoch [51/10000], Accuracy: 0.5081, Validation Loss: 1.1328\n",
      "up\n",
      "Epoch [52/10000], Accuracy: 0.5131, Validation Loss: 1.1308\n",
      "Epoch [53/10000], Accuracy: 0.5125, Validation Loss: 1.1283\n",
      "Epoch [54/10000], Accuracy: 0.5129, Validation Loss: 1.1316\n",
      "up\n",
      "Epoch [55/10000], Accuracy: 0.5131, Validation Loss: 1.1272\n",
      "Epoch [56/10000], Accuracy: 0.5116, Validation Loss: 1.1271\n",
      "Epoch [57/10000], Accuracy: 0.5148, Validation Loss: 1.1279\n",
      "up\n",
      "Epoch [58/10000], Accuracy: 0.5154, Validation Loss: 1.1245\n",
      "Epoch [59/10000], Accuracy: 0.5150, Validation Loss: 1.1238\n",
      "Epoch [60/10000], Accuracy: 0.5136, Validation Loss: 1.1229\n",
      "Epoch [61/10000], Accuracy: 0.5128, Validation Loss: 1.1228\n",
      "Epoch [62/10000], Accuracy: 0.5163, Validation Loss: 1.1207\n",
      "Epoch [63/10000], Accuracy: 0.5162, Validation Loss: 1.1216\n",
      "up\n",
      "Epoch [64/10000], Accuracy: 0.5140, Validation Loss: 1.1210\n",
      "up\n",
      "Epoch [65/10000], Accuracy: 0.5142, Validation Loss: 1.1210\n",
      "up\n",
      "Epoch [66/10000], Accuracy: 0.5174, Validation Loss: 1.1170\n",
      "Epoch [67/10000], Accuracy: 0.5185, Validation Loss: 1.1171\n",
      "up\n",
      "Epoch [68/10000], Accuracy: 0.5171, Validation Loss: 1.1165\n",
      "Epoch [69/10000], Accuracy: 0.5158, Validation Loss: 1.1180\n",
      "up\n",
      "Epoch [70/10000], Accuracy: 0.5178, Validation Loss: 1.1153\n",
      "Epoch [71/10000], Accuracy: 0.5211, Validation Loss: 1.1152\n",
      "Epoch [72/10000], Accuracy: 0.5209, Validation Loss: 1.1133\n",
      "Epoch [73/10000], Accuracy: 0.5203, Validation Loss: 1.1128\n",
      "Epoch [74/10000], Accuracy: 0.5197, Validation Loss: 1.1112\n",
      "Epoch [75/10000], Accuracy: 0.5200, Validation Loss: 1.1127\n",
      "up\n",
      "Epoch [76/10000], Accuracy: 0.5176, Validation Loss: 1.1120\n",
      "up\n",
      "Epoch [77/10000], Accuracy: 0.5221, Validation Loss: 1.1097\n",
      "Epoch [78/10000], Accuracy: 0.5212, Validation Loss: 1.1104\n",
      "up\n",
      "Epoch [79/10000], Accuracy: 0.5212, Validation Loss: 1.1088\n",
      "Epoch [80/10000], Accuracy: 0.5231, Validation Loss: 1.1079\n",
      "Epoch [81/10000], Accuracy: 0.5243, Validation Loss: 1.1076\n",
      "Epoch [82/10000], Accuracy: 0.5215, Validation Loss: 1.1069\n",
      "Epoch [83/10000], Accuracy: 0.5224, Validation Loss: 1.1096\n",
      "up\n",
      "Epoch [84/10000], Accuracy: 0.5233, Validation Loss: 1.1047\n",
      "Epoch [85/10000], Accuracy: 0.5233, Validation Loss: 1.1053\n",
      "up\n",
      "Epoch [86/10000], Accuracy: 0.5241, Validation Loss: 1.1058\n",
      "up\n",
      "Epoch [87/10000], Accuracy: 0.5237, Validation Loss: 1.1060\n",
      "up\n",
      "Epoch [88/10000], Accuracy: 0.5227, Validation Loss: 1.1051\n",
      "up\n",
      "Epoch [89/10000], Accuracy: 0.5221, Validation Loss: 1.1054\n",
      "up\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    \n",
    "    for text_input, audio_input, labels in train_dataloader:  # train_dataloader에 대한 반복\n",
    "        text_input = text_input.to(device)\n",
    "        audio_input = audio_input.to(device)\n",
    "        labels = labels.to(device).argmax(dim=1)\n",
    "        \n",
    "        optimizer.zero_grad()  # 그레이디언트 초기화\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = model(text_input, audio_input)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 검증 데이터셋을 사용하여 모델 평가\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for text_input, audio_input, labels in val_dataloader:  # val_dataloader에 대한 반복\n",
    "            text_input = text_input.to(device)\n",
    "            audio_input = audio_input.to(device)\n",
    "            labels = labels.to(device).argmax(dim=1)\n",
    "            \n",
    "            outputs = model(text_input, audio_input)\n",
    "            probabilities = F.softmax(outputs, dim=1)  # 소프트맥스 함수를 적용\n",
    "            _, predicted = torch.max(probabilities, dim=1)  # 확률값 중에서 가장 큰 값의 인덱스를 선택\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            val_loss += criterion(outputs, labels).item() * text_input.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    val_loss /= len(val_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 조기 종료 확인\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(\"up\")\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f1a12",
   "metadata": {},
   "source": [
    "52%의 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d34cf9",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40ae1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"model\\\\text_audio_model_52.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
